{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24e7443-3d1e-47db-b10f-a6e07515c321",
   "metadata": {},
   "source": [
    "This notebooks is created using Chapter 2 of the the [Advanced NLP with spaCy](https://course.spacy.io/en/chapter2) course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00edf023-0d9c-4c30-b81f-1dd12b45b65d",
   "metadata": {},
   "source": [
    "# Chapter 2: Large-scale data analysis with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7950db9-5caf-4759-b008-612ac73e25a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7643ca3a-6c9b-4f41-9e86-4ef5ab2eaa2d",
   "metadata": {},
   "source": [
    "### Shared vocab and string store (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ac448-5222-48e5-b554-993f0bb50871",
   "metadata": {},
   "source": [
    "spaCy stores all shared data in a vocabulary, the Vocab.\n",
    "\n",
    "This includes words, but also the labels schemes for tags and entities.\n",
    "\n",
    "To save memory, all strings are encoded to hash IDs. If a word occurs more than once, we don't need to save it every time.\n",
    "\n",
    "Instead, spaCy uses a hash function to generate an ID and stores the string only once in the string store. The string store is available as `nlp.vocab.strings`.\n",
    "\n",
    "It's a lookup table that works in both directions. You can look up a string and get its hash, and look up a hash to get its string value. Internally, spaCy only communicates in hash IDs.\n",
    "\n",
    "Hash IDs can't be reversed, though. If a word is not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067ffef-c630-4b68-a015-0986e9ea61b2",
   "metadata": {},
   "source": [
    "- `Vocab`: stores data shared across multiple documents\n",
    "- To save memory, spaCy encodes all strings to hash values\n",
    "- Strings are only stored once in the `StringStore` via `nlp.vocab.strings`\n",
    "- String store: **lookup** table in both directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f744e71f-ae83-43a6-b793-46d33489408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Create a blank English nlp object\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06cc4983-3346-4149-9e3e-4cf488b457f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings.add(\"coffee\")\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f84c16a-cf6d-47bb-91d1-62b9df494550",
   "metadata": {},
   "source": [
    "- Hashes can't be reversed ‚Äì that's why we need to provide the shared vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a638536-7b5b-4291-8097-957138fdc88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3197928453018144401"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c05acc8-1edc-423f-9e4b-5a44de6aa522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raises an error if we haven't seen the string before\n",
    "string = nlp.vocab.strings[3197928453018144401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed798870-d0b7-4e6a-b6d0-0cd216c27bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coffee'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0711877d-3bb9-4dd1-90d7-23b8c2c13759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.strings.StringStore at 0x7c4eb8ddc7b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13592a1-d93a-4e34-b880-747b44734d9c",
   "metadata": {},
   "source": [
    "### Shared vocab and string store (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996cf25-8045-4678-81b7-5aa07ff623af",
   "metadata": {},
   "source": [
    "To get the hash for a string, we can look it up in `nlp.vocab.strings`.\n",
    "\n",
    "To get the string representation of a hash, we can look up the hash.\n",
    "\n",
    "A `Doc` object also exposes its vocab and strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7181d1-7503-4521-9ab1-97cd69140baf",
   "metadata": {},
   "source": [
    "- Look up the string and hash in `nlp.vocab.strings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05b7eba-8e96-4d75-b9b5-8e3ff1251884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n",
      "string value: coffee\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7ef69-f2de-49a5-9743-a665a33fca7d",
   "metadata": {},
   "source": [
    "- The `doc` also exposes the vocab and strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd9235eb-a812-4736-b32c-68477fc49c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", doc.vocab.strings[\"coffee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67b0a53-0d0c-4bfa-b494-222e11efc766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3702023516439754181\n"
     ]
    }
   ],
   "source": [
    "print(\"hash value:\", doc.vocab.strings[\"love\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf7c09-68ed-452f-aeff-20a759f89725",
   "metadata": {},
   "source": [
    "### Lexemes: entries in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618ddaf-796a-4e72-9f6b-8e774b32becf",
   "metadata": {},
   "source": [
    "Lexemes are context-independent entries in the vocabulary.\n",
    "\n",
    "You can get a lexeme by looking up a string or a hash ID in the vocab.\n",
    "\n",
    "Lexemes expose attributes, just like tokens.\n",
    "\n",
    "They hold context-independent information about a word, like the text, or whether the word consists of alphabetic characters.\n",
    "\n",
    "Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4e08f-3cff-4451-b889-53a8574befbc",
   "metadata": {},
   "source": [
    "- A Lexeme object is an entry in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7030de62-ec36-4a48-bf51-3c5ba9404926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee0fe8-e5a0-4eef-84ea-fde31c9eb81b",
   "metadata": {},
   "source": [
    "- Contains the context-independent information about a word\n",
    "  - Word text: lexeme.text and lexeme.orth (the hash)\n",
    "  - Lexical attributes like lexeme.is_alpha\n",
    "  - Not context-dependent part-of-speech tags, dependencies or entity labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c505d4-4b16-4514-a325-c751e086c0fa",
   "metadata": {},
   "source": [
    "### Vocab, hashes and lexemes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7965f-760d-45b6-b9b8-d5fc61c05eca",
   "metadata": {},
   "source": [
    "Here's an example.\n",
    "\n",
    "The Doc contains words in context ‚Äì in this case, the tokens \"I\", \"love\" and \"coffee\" with their part-of-speech tags and dependencies.\n",
    "\n",
    "Each token refers to a lexeme, which knows the word's hash ID. To get the string representation of the word, spaCy looks up the hash in the string store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ea666-8a4f-4c71-8c5d-41cf619e0cdf",
   "metadata": {},
   "source": [
    "![Vocab, hashes and lexemes](./img/vocab_stringstore.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0d1e5-edf3-44f8-a678-13986bb1c736",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Strings to hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a364a9-3796-4314-a051-f2681cb1f899",
   "metadata": {},
   "source": [
    "Part 1\n",
    "- Look up the string ‚Äúcat‚Äù in `nlp.vocab.strings` to get the hash.\n",
    "- Look up the hash to get back the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73842e2d-3a57-48e4-a411-8e2f80e3b584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ffa8e-7c1e-4585-8224-488bcbf076c2",
   "metadata": {},
   "source": [
    "Part 2\n",
    "- Look up the string label ‚ÄúPERSON‚Äù in `nlp.vocab.strings` to get the hash.\n",
    "- Look up the hash to get back the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65bde95c-b31b-4e82-bf5e-fe0cc1d06e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dcdf74-24cf-4ba3-a646-cd4d619ef62f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vocab, hashes and lexemes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb59d6f-dcc7-425d-9dd2-1052f6dba165",
   "metadata": {},
   "source": [
    "Why does this code throw an error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ac820c5-2ff1-4853-8b6c-9bc5b27814c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2644858412616767388\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"[E018] Can't retrieve string for hash '2644858412616767388'. This usually refers to an issue with the `Vocab` or `StringStore`.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(bowie_id)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Look up the ID for \"Bowie\" in the vocab\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(nlp_de\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstrings[bowie_id])\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-spacy/lib/python3.12/site-packages/spacy/strings.pyx:162\u001b[0m, in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '2644858412616767388'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create an English and German nlp object\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp_de = spacy.blank(\"de\")\n",
    "\n",
    "# Get the ID for the string 'Bowie'\n",
    "bowie_id = nlp.vocab.strings[\"Bowie\"]\n",
    "print(bowie_id)\n",
    "\n",
    "# Look up the ID for \"Bowie\" in the vocab\n",
    "print(nlp_de.vocab.strings[bowie_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c465c3-6e26-4b66-a158-8a7fd359681b",
   "metadata": {},
   "source": [
    "The string \"Bowie\" isn‚Äôt in the German vocab, so the hash can‚Äôt be resolved in the string store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f11157-e3bb-4433-ae90-901800a52f1f",
   "metadata": {},
   "source": [
    "Hashes can‚Äôt be reversed. To prevent this problem, add the word to the new vocab by processing a text or looking up the string, or use the same vocab to resolve the hash back to a string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e13b30-e861-4d88-a446-7f710d31d0c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Structures(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd364a-1d97-4d43-8cfd-d963a0796f8f",
   "metadata": {},
   "source": [
    "Now that you know all about the vocabulary and string store, we can take a look at the most important data structure: the `Doc`, and its views `Token` and `Span`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc1cea-7d42-497a-bc6b-fbd5f9d424b4",
   "metadata": {},
   "source": [
    "### The Doc object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79e78a-cbf1-47b4-8ede-8f67dbcd2022",
   "metadata": {},
   "source": [
    "The `Doc` is one of the central data structures in spaCy. It's created automatically when you process a text with the `nlp` object. But you can also instantiate the class manually.\n",
    "\n",
    "After creating the `nlp` object, we can import the `Doc` class from `spacy.tokens`.\n",
    "\n",
    "Here we're creating a doc from three words. The spaces are a list of boolean values indicating whether the word is followed by a space. Every token includes that information ‚Äì even the last one!\n",
    "\n",
    "The `Doc` class takes three arguments: the shared vocab, the words and the spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "367ced5f-faf2-406a-a00b-fcce176d8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024dddb1-dd57-446d-81c2-f2168bbebe71",
   "metadata": {},
   "source": [
    "### The Span object(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6720eed-4476-42a6-ae8e-fdb94a287f9c",
   "metadata": {},
   "source": [
    "A `Span` is a slice of a doc consisting of one or more tokens. The `Span` takes at least three arguments: the doc it refers to, and the start and end index of the span. Remember that the end index is exclusive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ccadda-119e-4e83-ae90-3a2c4e2f1aaa",
   "metadata": {},
   "source": [
    "![The Span Object](./img/span_indices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6892d8-7829-4c09-9568-66081568dfea",
   "metadata": {},
   "source": [
    "### The Span object(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16d26e-b49b-46e4-a60b-841eced130db",
   "metadata": {},
   "source": [
    "To create a `Span` manually, we can also import the class from `spacy.tokens`. We can then instantiate it with the doc and the span's start and end index, and an optional label argument.\n",
    "\n",
    "The `doc.ents` are writable, so we can add entities manually by overwriting it with a list of spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "145983e4-d14e-42e6-98fb-d6786b730655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0357cc-8fc3-40bd-ab3e-769b2c81dbef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29184d-f3cd-43ef-82b0-60c84c6eb31d",
   "metadata": {},
   "source": [
    "A few tips and tricks before we get started:\n",
    "\n",
    "The `Doc` and `Span` are very powerful and optimized for performance. They give you access to all references and relationships of the words and sentences.\n",
    "\n",
    "If your application needs to output strings, make sure to convert the doc as late as possible. If you do it too early, you'll lose all relationships between the tokens.\n",
    "\n",
    "To keep things consistent, try to use built-in token attributes wherever possible. For example, `token.i` for the token index.\n",
    "\n",
    "Also, don't forget to always pass in the shared vocab!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b28ce-31ea-44e7-bd5e-5ba9c174e5eb",
   "metadata": {},
   "source": [
    "- `Doc` and `Span` are very powerful and hold references and relationships of words and sentences\n",
    "  - **Convert result to strings as late as possible**\n",
    "  - **Use token attributes if available** ‚Äì for example, `token.i` for the token index\n",
    "  \n",
    "- Don't forget to pass in the shared `vocab`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a818b-687e-47b9-a87d-c8cf8e3e9d3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Creating a Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f5f97-0e62-4858-8655-28791df83c45",
   "metadata": {},
   "source": [
    "Let‚Äôs create some Doc objects from scratch!\n",
    "\n",
    "Part 1\n",
    "\n",
    "Import the `Doc` from `spacy.tokens`.\n",
    "\n",
    "Create a `Doc` from the `words` and `spaces`. Don‚Äôt forget to pass in the vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df67c7a-8afc-45ab-9e95-1e03c97a1277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6ab8f-f1b5-48f5-9a57-27bf2ad9fe5d",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n",
    "Import the `Doc` from `spacy.tokens`.\n",
    "\n",
    "Create a `Doc` from the `words` and `spaces`. Don‚Äôt forget to pass in the vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76808128-b262-4eea-b63a-3643767aa9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab329f6e-bc5f-4d99-8dac-024d06e8b797",
   "metadata": {},
   "source": [
    "Part 3\n",
    "\n",
    "Import the `Doc` from `spacy.tokens`.\n",
    "\n",
    "Complete the `words` and `spaces` to match the desired text and create a `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10b134d7-421d-4184-9603-bb4067a8d035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Oh, really?!\"\n",
    "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f99a8-895f-4fd0-9e21-1ac1e01bdb14",
   "metadata": {},
   "source": [
    "## Docs, spans and entities from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdadf4c1-a1f2-4d70-8be3-4991f06235f0",
   "metadata": {},
   "source": [
    "In this exercise, you‚Äôll create the `Doc` and `Span` objects manually, and update the named entities ‚Äì just like spaCy does behind the scenes. A shared `nlp` object has already been created.\n",
    "\n",
    "- Import the `Doc` and `Span` classes from `spacy.tokens`.\n",
    "- Use the `Doc` class directly to create a `doc` from the words and spaces.\n",
    "- Create a `Span` for ‚ÄúDavid Bowie‚Äù from the `doc` and assign it the label `\"PERSON\"`.\n",
    "- Overwrite the `doc.ents` with a list of one entity, the ‚ÄúDavid Bowie‚Äù `span`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ddade1-895c-4769-b0f3-570ce7f38c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0123036-0282-48c7-828d-3bb30c84b769",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data structures best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65f6a7-85fb-4ee3-ab19-2410e0085734",
   "metadata": {},
   "source": [
    "The code in this example is trying to analyze a text and collect all proper nouns that are followed by a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d11d2f-314e-4e73-9513-2550d692a26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28e74c-68ee-4517-b7da-7adbb7e3078a",
   "metadata": {},
   "source": [
    "Why is the code bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aec3bd-d002-4b49-9548-55947400e76b",
   "metadata": {},
   "source": [
    "It only uses lists of strings instead of native token attributes. This is often less efficient, and can't express complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64afee5c-41f6-4a6c-95de-bc032d93a679",
   "metadata": {},
   "source": [
    "That's correct! Always convert the results to strings as late as possible, and try to use native token attributes to keep things consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5e69b-0220-4a1c-909d-4d7870ce2be3",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n",
    "Rewrite the code to use the native token attributes instead of lists of `token_texts` and `pos_tags`.\n",
    "\n",
    "Loop over each `token` in the `doc` and check the `token.pos_` attribute.\n",
    "\n",
    "Use `doc[token.i + 1]` to check for the next token and its `.pos_` attribute.\n",
    "\n",
    "If a proper noun before a verb is found, print its `token.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b82384d2-d59c-4311-bf4a-d1d48e089985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"Berlin looks like a nice city\")\n",
    "doc = nlp(\"Berlin looks like a nice city Berlin\")\n",
    "\n",
    "doc_len = len(doc)\n",
    "\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if token.i + 1 < doc_len: # If the doc ends with a proper noun, doc[token.i + 1] will fail\n",
    "            if doc[token.i + 1].pos_ == \"VERB\":\n",
    "                print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b5fcc-4be0-4bd2-8137-7060c8387a00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Word vectors and semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f0e2b-b6ff-413d-8ab0-e85b52b776b6",
   "metadata": {},
   "source": [
    "### Comparing semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26b0bf-0dac-4bd2-b3fb-e4c78c1c87d5",
   "metadata": {},
   "source": [
    "spaCy can compare two objects and predict how similar they are ‚Äì for example, documents, spans or single tokens.\n",
    "\n",
    "The `Doc`, `Token` and `Span` objects have a `.similarity` method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are.\n",
    "\n",
    "One thing that's very important: In order to use similarity, you need a larger spaCy pipeline that has word vectors included.\n",
    "\n",
    "For example, the medium or large English pipeline ‚Äì but not the small one. So if you want to use vectors, always go with a pipeline that ends in \"md\" or \"lg\". You can find more details on this in the [documentation](https://spacy.io/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaadc3d-34e8-4eee-90bf-ecee0d0ef96f",
   "metadata": {},
   "source": [
    "- spaCy can compare two objects and predict similarity\n",
    "- Doc.similarity(), Span.similarity() and Token.similarity()\n",
    "- Take another object and return a similarity score (0 to 1)\n",
    "- Important: needs a pipeline that has word vectors included, for example:\n",
    "  - ‚úÖ en_core_web_md (medium)\n",
    "  - ‚úÖ en_core_web_lg (large)\n",
    "  - üö´ NOT en_core_web_sm (small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41f081-8f77-428e-ab98-18a2a8a8a39b",
   "metadata": {},
   "source": [
    "### Similarity examples (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445192a-1cba-4831-8235-2132f2b821f8",
   "metadata": {},
   "source": [
    "Here's an example. Let's say we want to find out whether two documents are similar.\n",
    "\n",
    "First, we load the medium English pipeline, \"en_core_web_md\".\n",
    "\n",
    "We can then create two doc objects and use the first doc's similarity method to compare it to the second.\n",
    "\n",
    "Here, a fairly high similarity score of 0.86 is predicted for \"I like fast food\" and \"I like pizza\".\n",
    "\n",
    "The same works for tokens.\n",
    "\n",
    "According to the word vectors, the tokens \"pizza\" and \"pasta\" are kind of similar, and receive a score of 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac940607-6e28-4bc2-aef9-e674037351da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8382381200790405\n"
     ]
    }
   ],
   "source": [
    "# Load a larger pipeline with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a5d1064-2110-40a8-9c30-e331c550c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825aefb-7c24-4f34-97c2-d1aed9fb33ad",
   "metadata": {},
   "source": [
    "### Similarity examples (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea850afe-db08-4d80-b70f-fccb4e62b75e",
   "metadata": {},
   "source": [
    "You can also use the `similarity` methods to compare different types of objects.\n",
    "\n",
    "For example, a document and a token.\n",
    "\n",
    "Here, the similarity score is pretty low and the two objects are considered fairly dissimilar.\n",
    "\n",
    "Here's another example comparing a span ‚Äì \"pizza and pasta\" ‚Äì to a document about McDonalds.\n",
    "\n",
    "The score returned here is 0.61, so it's determined to be kind of similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0481a1d-bce9-49fa-b152-abeb9ff8e54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2274085134267807\n"
     ]
    }
   ],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b1b758a-92e5-4b57-b70a-5276f93202ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5528545379638672\n"
     ]
    }
   ],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73864a44-331c-4f44-844a-f737024f3a99",
   "metadata": {},
   "source": [
    "### How does spaCy predict similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8686d-667b-4ba5-8349-e947fb9fd430",
   "metadata": {},
   "source": [
    "But how does spaCy do this under the hood?\n",
    "\n",
    "Similarity is determined using word vectors, multi-dimensional representations of meanings of words.\n",
    "\n",
    "You might have heard of Word2Vec, which is an algorithm that's often used to train word vectors from raw text.\n",
    "\n",
    "Vectors can be added to spaCy's pipelines.\n",
    "\n",
    "By default, the similarity returned by spaCy is the cosine similarity between two vectors ‚Äì but this can be adjusted if necessary.\n",
    "\n",
    "Vectors for objects consisting of several tokens, like the `Doc` and `Span`, default to the average of their token vectors.\n",
    "\n",
    "That's also why you usually get more value out of shorter phrases with fewer irrelevant words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7a3f3-1218-47c7-8016-a2361e742cd8",
   "metadata": {},
   "source": [
    "- Similarity is determined using word vectors\n",
    "- Multi-dimensional meaning representations of words\n",
    "- Generated using an algorithm like [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) and lots of text\n",
    "- Can be added to spaCy's pipelines\n",
    "- Default: cosine similarity, but can be adjusted\n",
    "- `Doc` and `Span` vectors default to average of token vectors\n",
    "- Short phrases are better than long documents with many irrelevant words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787bf79-d372-43ce-9a84-b4664495ac2d",
   "metadata": {},
   "source": [
    "### Word vectors in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159cb4c-3047-4b5c-843d-583a82a3ef68",
   "metadata": {},
   "source": [
    "To give you an idea of what those vectors look like, here's an example.\n",
    "\n",
    "First, we load the medium pipeline again, which ships with word vectors.\n",
    "\n",
    "Next, we can process a text and look up a token's vector using the `.vector` attribute.\n",
    "\n",
    "The result is a 300-dimensional vector of the word \"banana\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1df1be06-98d8-4abd-99c1-ab421f575f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6334     0.18981   -0.53544   -0.52658   -0.30001    0.30559\n",
      " -0.49303    0.14636    0.012273   0.96802    0.0040354  0.25234\n",
      " -0.29864   -0.014646  -0.24905   -0.67125   -0.053366   0.59426\n",
      " -0.068034   0.10315    0.66759    0.024617  -0.37548    0.52557\n",
      "  0.054449  -0.36748   -0.28013    0.090898  -0.025687  -0.5947\n",
      " -0.24269    0.28603    0.686      0.29737    0.30422    0.69032\n",
      "  0.042784   0.023701  -0.57165    0.70581   -0.20813   -0.03204\n",
      " -0.12494   -0.42933    0.31271    0.30352    0.09421   -0.15493\n",
      "  0.071356   0.15022   -0.41792    0.066394  -0.034546  -0.45772\n",
      "  0.57177   -0.82755   -0.27885    0.71801   -0.12425    0.18551\n",
      "  0.41342   -0.53997    0.55864   -0.015805  -0.1074    -0.29981\n",
      " -0.17271    0.27066    0.043996   0.60107   -0.353      0.6831\n",
      "  0.20703    0.12068    0.24852   -0.15605    0.25812    0.007004\n",
      " -0.10741   -0.097053   0.085628   0.096307   0.20857   -0.23338\n",
      " -0.077905  -0.030906   1.0494     0.55368   -0.10703    0.052234\n",
      "  0.43407   -0.13926    0.38115    0.021104  -0.40922    0.35972\n",
      " -0.28898    0.30618    0.060807  -0.023517   0.58193   -0.3098\n",
      "  0.21013   -0.15557   -0.56913   -1.1364     0.36598   -0.032666\n",
      "  1.1926     0.12825   -0.090486  -0.47965   -0.61164   -0.16484\n",
      " -0.41134    0.19925    0.059183  -0.20842    0.45223    0.27697\n",
      " -0.20745    0.025404  -0.28874    0.040478  -0.22275   -0.43323\n",
      "  0.76957   -0.054327  -0.35213   -0.30842   -0.48791   -0.35564\n",
      "  0.19813   -0.094767  -0.50918    0.18763   -0.087555   0.37709\n",
      " -0.1322    -0.096913  -1.9102     0.55813    0.27391   -0.077744\n",
      " -0.43933   -0.10367   -0.24408    0.41869    0.11659    0.27454\n",
      "  0.81021   -0.11006    0.43131    0.29095   -0.49548   -0.31958\n",
      " -0.072506   0.020286   0.2179     0.22032   -0.29212    0.75639\n",
      "  0.13598    0.019736  -0.83104    0.22836   -0.28669   -1.0529\n",
      "  0.052771   0.41266    0.50149    0.5323     0.51573   -0.31806\n",
      " -0.4619     0.21739   -0.43584   -0.41382    0.042237  -0.57179\n",
      "  0.067623  -0.27854    0.090044   0.20633    0.024678  -0.57703\n",
      " -0.020183  -0.53147   -0.37548   -0.12795   -0.093662  -0.0061183\n",
      "  0.20221   -0.62296   -0.29746    0.26935    0.59009   -0.50382\n",
      " -0.69757    0.20157   -0.33592   -0.45766    0.14061    0.22982\n",
      "  0.044046   0.26386    0.02942    0.34095    1.1496    -0.15555\n",
      " -0.064071   0.30139    0.024211  -0.63515   -0.73347   -0.10346\n",
      " -0.22637   -0.056392  -0.16735   -0.097331  -0.19206   -0.18866\n",
      "  0.15116   -0.038048   0.70205    0.11586   -0.14813    0.0095166\n",
      " -0.33804   -0.10158   -0.23829   -0.22759    0.092504  -0.29839\n",
      " -0.39721    0.26092    0.34594   -0.47396   -0.25725   -0.19257\n",
      " -0.53071    0.1692    -0.47252   -0.17333   -0.40505    0.046446\n",
      " -0.04473    0.33555   -0.5693     0.31591   -0.21167   -0.31298\n",
      " -0.45923   -0.083091   0.086822   0.01264    0.43779    0.12651\n",
      "  0.30156    0.022061   0.26549   -0.29455   -0.14838    0.033692\n",
      " -0.37346   -0.075343  -0.56498   -0.24207   -0.69351   -0.20277\n",
      " -0.0081185  0.030971   0.53615   -0.16613   -0.84087    0.74661\n",
      "  0.029132   0.46936   -0.49755    0.40954   -0.022558   0.21497\n",
      " -0.049528  -0.039799   0.46165    0.26456    0.32985   -0.04219\n",
      " -0.099599  -0.17312   -0.476     -0.019048  -0.41888   -0.2685\n",
      " -0.65281    0.068773  -0.23881   -1.1784     0.25504    0.61171  ]\n"
     ]
    }
   ],
   "source": [
    "# Load a larger pipeline with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd5844e-f682-48c7-ab24-78d256f85a36",
   "metadata": {},
   "source": [
    "### Similarity depends on the application context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6dbb6-b955-42fd-9246-bfe35298e655",
   "metadata": {},
   "source": [
    "Predicting similarity can be useful for many types of applications. For example, to recommend a user similar texts based on the ones they have read. It can also be helpful to flag duplicate content, like posts on an online platform.\n",
    "\n",
    "However, it's important to keep in mind that there's no objective definition of what's similar and what isn't. It always depends on the context and what your application needs to do.\n",
    "\n",
    "Here's an example: spaCy's default word vectors assign a very high similarity score to \"I like cats\" and \"I hate cats\". This makes sense, because both texts express sentiment about cats. But in a different application context, you might want to consider the phrases as very dissimilar, because they talk about opposite sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e1f95-fc63-481f-8378-e44245937a7e",
   "metadata": {},
   "source": [
    "- Useful for many applications: recommendation systems, flagging duplicates etc.\n",
    "- There's no objective definition of \"similarity\"\n",
    "- Depends on the context and what application needs to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44f0b47e-49fe-4fd5-bde3-8c4fb7d83dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332ade0-92c1-4306-a683-5275a5758b37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inspecting word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a322c0c-eba0-41cd-830a-2a2b0c2fd287",
   "metadata": {},
   "source": [
    "In this exercise, you‚Äôll use a larger [English pipeline](https://spacy.io/models/en), which includes around 20.000 word vectors. The pipeline package is already pre-installed.\n",
    "\n",
    "- Load the medium `\"en_core_web_md\"` pipeline with word vectors.\n",
    "- Print the vector for `\"bananas\"` using the `token.vector` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44fd45b0-af3a-4694-9e73-7a9f96443099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6334     0.18981   -0.53544   -0.52658   -0.30001    0.30559\n",
      " -0.49303    0.14636    0.012273   0.96802    0.0040354  0.25234\n",
      " -0.29864   -0.014646  -0.24905   -0.67125   -0.053366   0.59426\n",
      " -0.068034   0.10315    0.66759    0.024617  -0.37548    0.52557\n",
      "  0.054449  -0.36748   -0.28013    0.090898  -0.025687  -0.5947\n",
      " -0.24269    0.28603    0.686      0.29737    0.30422    0.69032\n",
      "  0.042784   0.023701  -0.57165    0.70581   -0.20813   -0.03204\n",
      " -0.12494   -0.42933    0.31271    0.30352    0.09421   -0.15493\n",
      "  0.071356   0.15022   -0.41792    0.066394  -0.034546  -0.45772\n",
      "  0.57177   -0.82755   -0.27885    0.71801   -0.12425    0.18551\n",
      "  0.41342   -0.53997    0.55864   -0.015805  -0.1074    -0.29981\n",
      " -0.17271    0.27066    0.043996   0.60107   -0.353      0.6831\n",
      "  0.20703    0.12068    0.24852   -0.15605    0.25812    0.007004\n",
      " -0.10741   -0.097053   0.085628   0.096307   0.20857   -0.23338\n",
      " -0.077905  -0.030906   1.0494     0.55368   -0.10703    0.052234\n",
      "  0.43407   -0.13926    0.38115    0.021104  -0.40922    0.35972\n",
      " -0.28898    0.30618    0.060807  -0.023517   0.58193   -0.3098\n",
      "  0.21013   -0.15557   -0.56913   -1.1364     0.36598   -0.032666\n",
      "  1.1926     0.12825   -0.090486  -0.47965   -0.61164   -0.16484\n",
      " -0.41134    0.19925    0.059183  -0.20842    0.45223    0.27697\n",
      " -0.20745    0.025404  -0.28874    0.040478  -0.22275   -0.43323\n",
      "  0.76957   -0.054327  -0.35213   -0.30842   -0.48791   -0.35564\n",
      "  0.19813   -0.094767  -0.50918    0.18763   -0.087555   0.37709\n",
      " -0.1322    -0.096913  -1.9102     0.55813    0.27391   -0.077744\n",
      " -0.43933   -0.10367   -0.24408    0.41869    0.11659    0.27454\n",
      "  0.81021   -0.11006    0.43131    0.29095   -0.49548   -0.31958\n",
      " -0.072506   0.020286   0.2179     0.22032   -0.29212    0.75639\n",
      "  0.13598    0.019736  -0.83104    0.22836   -0.28669   -1.0529\n",
      "  0.052771   0.41266    0.50149    0.5323     0.51573   -0.31806\n",
      " -0.4619     0.21739   -0.43584   -0.41382    0.042237  -0.57179\n",
      "  0.067623  -0.27854    0.090044   0.20633    0.024678  -0.57703\n",
      " -0.020183  -0.53147   -0.37548   -0.12795   -0.093662  -0.0061183\n",
      "  0.20221   -0.62296   -0.29746    0.26935    0.59009   -0.50382\n",
      " -0.69757    0.20157   -0.33592   -0.45766    0.14061    0.22982\n",
      "  0.044046   0.26386    0.02942    0.34095    1.1496    -0.15555\n",
      " -0.064071   0.30139    0.024211  -0.63515   -0.73347   -0.10346\n",
      " -0.22637   -0.056392  -0.16735   -0.097331  -0.19206   -0.18866\n",
      "  0.15116   -0.038048   0.70205    0.11586   -0.14813    0.0095166\n",
      " -0.33804   -0.10158   -0.23829   -0.22759    0.092504  -0.29839\n",
      " -0.39721    0.26092    0.34594   -0.47396   -0.25725   -0.19257\n",
      " -0.53071    0.1692    -0.47252   -0.17333   -0.40505    0.046446\n",
      " -0.04473    0.33555   -0.5693     0.31591   -0.21167   -0.31298\n",
      " -0.45923   -0.083091   0.086822   0.01264    0.43779    0.12651\n",
      "  0.30156    0.022061   0.26549   -0.29455   -0.14838    0.033692\n",
      " -0.37346   -0.075343  -0.56498   -0.24207   -0.69351   -0.20277\n",
      " -0.0081185  0.030971   0.53615   -0.16613   -0.84087    0.74661\n",
      "  0.029132   0.46936   -0.49755    0.40954   -0.022558   0.21497\n",
      " -0.049528  -0.039799   0.46165    0.26456    0.32985   -0.04219\n",
      " -0.099599  -0.17312   -0.476     -0.019048  -0.41888   -0.2685\n",
      " -0.65281    0.068773  -0.23881   -1.1784     0.25504    0.61171  ]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_md pipeline\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41382331-5c42-4a11-9153-28f9e20c0652",
   "metadata": {},
   "source": [
    "## Comparing similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c71815-2d09-4ebf-a145-dc628e1f77ca",
   "metadata": {},
   "source": [
    "In this exercise, you‚Äôll be using spaCy‚Äôs `similarity` methods to compare `Doc`, `Token` and `Span` objects and get similarity scores.\n",
    "\n",
    "Part 1\n",
    "\n",
    "- Use the `doc.similarity` method to compare `doc1` to `doc2` and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24a4725f-1acf-452a-a26d-d471aae66d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8456854224205017\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df4f98-9bf3-4544-a94f-5657e4737676",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n",
    "- Use the `token.similarity` method to compare `token1` to `token2` and print the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "374e13fa-b86c-40d3-9f62-eba4415c3bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18317238986492157\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63edb124-15af-49a6-99c3-eb822085ba8c",
   "metadata": {},
   "source": [
    "Part 3\n",
    "\n",
    "- Create spans for ‚Äúgreat restaurant‚Äù/‚Äúreally nice bar‚Äù.\n",
    "\n",
    "- Use `span.similarity` to compare them and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35c1b6b2-23e9-4099-89da-c9f632fa80ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7541285157203674\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[-4:-1]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c172b791-2905-4aac-a915-9319797c2ff4",
   "metadata": {},
   "source": [
    "## Combining predictions and rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c619236-c932-4d36-9fca-5f8b1dc85f9c",
   "metadata": {},
   "source": [
    "Combining predictions from statistical models with rule-based systems is one of the most powerful tricks you should have in your NLP toolbox.\n",
    "\n",
    "In this lesson, we'll take a look at how to do it with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019edef-1941-43e6-8c39-0cbedaa721e0",
   "metadata": {},
   "source": [
    "### Statistical predictions vs. rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b44f61-8ce9-41ee-934e-a3fb310bb3e6",
   "metadata": {},
   "source": [
    "Statistical models are useful if your application needs to be able to generalize based on a few examples.\n",
    "\n",
    "For instance, detecting product or person names usually benefits from a trained model. Instead of providing a list of all person names ever, your application will be able to predict whether a span of tokens is a person name. Similarly, you can predict dependency labels to find subject/object relationships.\n",
    "\n",
    "To do this, you would use spaCy's entity recognizer, dependency parser or part-of-speech tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1a646-bc60-4185-b35a-a782eda49a3a",
   "metadata": {},
   "source": [
    "|    |Statistical models | Rule-based systems|\n",
    "----|-------------------|-------------------|\n",
    "|Use cases\t|application needs to generalize based on examples| dictionary with finite number of examples |\n",
    "|Real-world examples|\tproduct names, person names, subject/object relationships| countries of the world, cities, drug names, dog breeds |\n",
    "|spaCy features|\tentity recognizer, dependency parser, part-of-speech tagger| tokenizer, Matcher, PhraseMatcher |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a0638-04ea-42bf-a497-2cd9834ccec1",
   "metadata": {},
   "source": [
    "Rule-based approaches on the other hand come in handy if there's a more or less finite number of instances you want to find. For example, all countries or cities of the world, drug names or even dog breeds.\n",
    "\n",
    "In spaCy, you can achieve this with custom tokenization rules, as well as the matcher and phrase matcher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edeafae-9771-4b1d-ac82-ed3310ca9619",
   "metadata": {},
   "source": [
    "### Recap: Rule-based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5217f3-daf0-491f-bbf2-c0277a43f651",
   "metadata": {},
   "source": [
    "In the last chapter, you learned how to use spaCy's rule-based matcher to find complex patterns in your texts. Here's a quick recap.\n",
    "\n",
    "The matcher is initialized with the shared vocabulary ‚Äì usually `nlp.vocab`.\n",
    "\n",
    "Patterns are lists of dictionaries, and each dictionary describes one token and its attributes. Patterns can be added to the matcher using the `matcher.add` method.\n",
    "\n",
    "Operators let you specify how often to match a token. For example, \"+\" will match one or more times.\n",
    "\n",
    "Calling the matcher on a doc object will return a list of the matches. Each match is a tuple consisting of an ID, and the start and end token index in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c421127-20e7-4797-bac3-19eef33740de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"LOWER\": \"cats\"}]\n",
    "matcher.add(\"LOVE_CATS\", [pattern])\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{\"TEXT\": \"very\", \"OP\": \"+\"}, {\"TEXT\": \"happy\"}]\n",
    "matcher.add(\"VERY_HAPPY\", [pattern])\n",
    "\n",
    "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c25e5f-952b-4f93-b11f-1fea53cd8077",
   "metadata": {},
   "source": [
    "### Adding statistical predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3e0bb-3ee9-4f13-a8d8-aacb07b2c574",
   "metadata": {},
   "source": [
    "Here's an example of a matcher rule for \"golden retriever\".\n",
    "\n",
    "If we iterate over the matches returned by the matcher, we can get the match ID and the start and end index of the matched span. We can then find out more about it. `Span` objects give us access to the original document and all other token attributes and linguistic features predicted by a model.\n",
    "\n",
    "For example, we can get the span's root token. If the span consists of more than one token, this will be the token that decides the category of the phrase. For example, the root of \"Golden Retriever\" is \"Retriever\". We can also find the head token of the root. This is the syntactic \"parent\" that governs the phrase ‚Äì in this case, the verb \"have\".\n",
    "\n",
    "Finally, we can look at the previous token and its attributes. In this case, it's a determiner, the article \"a\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae80dbc5-bab2-4295-b921-4f0959be102e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7047e12-d87e-4451-92ef-d654f7ded3bb",
   "metadata": {},
   "source": [
    "### Efficient phrase matching (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897df5c1-1c19-4b03-aa2c-bed9a20bf93d",
   "metadata": {},
   "source": [
    "The phrase matcher is another helpful tool to find sequences of words in your data.\n",
    "\n",
    "It performs a keyword search on the document, but instead of only finding strings, it gives you direct access to the tokens in context.\n",
    "\n",
    "It takes `Doc` objects as patterns.\n",
    "\n",
    "It's also really fast.\n",
    "\n",
    "This makes it very useful for matching large dictionaries and word lists on large volumes of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e260851-0e21-431f-856a-c443e34bb15c",
   "metadata": {},
   "source": [
    "- `PhraseMatcher` like regular expressions or keyword search ‚Äì but with access to the tokens!\n",
    "- Takes `Doc` object as patterns\n",
    "- More efficient and faster than the `Matcher`\n",
    "- Great for matching large word lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10184dc6-da3b-4c2d-b2dc-83b930634239",
   "metadata": {},
   "source": [
    "### Efficient phrase matching (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec7701-35d0-4f4b-97ab-05b5e109d48b",
   "metadata": {},
   "source": [
    "Here's an example.\n",
    "\n",
    "The phrase matcher can be imported from `spacy.matcher` and follows the same API as the regular matcher.\n",
    "\n",
    "Instead of a list of dictionaries, we pass in a `Doc` object as the pattern.\n",
    "\n",
    "We can then iterate over the matches in the text, which gives us the match ID, and the start and end of the match. This lets us create a Span object for the matched tokens \"Golden Retriever\" to analyze it in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57f987b0-01ef-4a98-a306-ba4b24452b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", [pattern])\n",
    "doc = nlp(\"I have a Folden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d77ed5-bbc1-4d60-bd26-1c0c9db04365",
   "metadata": {},
   "source": [
    "## Debugging patterns(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d0646-c858-4c2b-a9af-f393c545ef91",
   "metadata": {},
   "source": [
    "Why does this pattern not match the tokens ‚ÄúSilicon Valley‚Äù in the doc?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0981884-9edf-42b0-87c4-3b283a4a0de1",
   "metadata": {},
   "source": [
    "pattern = [{\"LOWER\": \"silicon\"}, {\"TEXT\": \" \"}, {\"LOWER\": \"valley\"}]\n",
    "\n",
    "doc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d30a0-1a86-48ca-a0e0-8151f13ada6b",
   "metadata": {},
   "source": [
    "- The tokens \"Silicon\" and \"Valley\" are not lowercase, so the \"LOWER\" attribute won‚Äôt match.\n",
    "\n",
    "- The tokenizer doesn‚Äôt create tokens for single spaces, so there‚Äôs no token with the value \" \" in between. **CORRECT**\n",
    "\n",
    "- The tokens are missing an operator \"OP\" to indicate that they should be matched exactly once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6c324-051f-4f56-bbae-74b013a9ce21",
   "metadata": {},
   "source": [
    "## Debugging patterns(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6813e-01f9-4c82-84ca-1574f2079a61",
   "metadata": {},
   "source": [
    "Both patterns in this exercise contain mistakes and won‚Äôt match as expected. Can you fix them? If you get stuck, try printing the tokens in the `doc` to see how the text will be split and adjust the pattern so that each dictionary represents one token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dfd27c-cd26-4fa0-91c9-9ac746d2d577",
   "metadata": {},
   "source": [
    "- Edit `pattern1` so that it correctly matches all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n",
    "- Edit `pattern2` so that it correctly matches all case-insensitive mentions of \"ad-free\", plus the following noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66c77da5-0913-4d09-b312-759c2cb52e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "#pattern1 = [{\"LOWER\": \"Amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "\n",
    "# pattern2 = [{\"LOWER\": \"ad-free\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98dcb006-623a-416e-8777-505297555de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad', '-', 'free', 'viewing']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code reveals the issue with the original pattern2; ad-free is matched as three sep. tokens\n",
    "[token.text for token in nlp(\"ad-free viewing\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90839a0e-7ffc-49d9-bbab-12b32b925f52",
   "metadata": {},
   "source": [
    "## Efficient phrase matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f6194-6969-4951-bc48-92cf783c04b8",
   "metadata": {},
   "source": [
    "Sometimes it‚Äôs more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things ‚Äì like all countries of the world. We already have a list of countries, so let‚Äôs use this as the basis of our information extraction script. A list of string names is available as the variable `COUNTRIES`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d7552-a02d-400a-8320-440ec80a5415",
   "metadata": {},
   "source": [
    "- Import the `PhraseMatcher` and initialize it with the shared `vocab` as the variable `matcher`.\n",
    "- Add the phrase patterns and call the matcher on the `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "944d8bc3-4ea9-4ff1-bd7b-7b626bcac927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "with open(\"../../exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752528a9-2dfb-460d-af05-83c294c7e94a",
   "metadata": {},
   "source": [
    "## Extracting countries and relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6cfa0-58c8-4e39-b630-2c4be9bba116",
   "metadata": {},
   "source": [
    "In the previous exercise, you wrote a script using spaCy‚Äôs `PhraseMatcher` to find country names in text. Let‚Äôs use that country matcher on a longer text, analyze the syntax and update the document‚Äôs entities with the matched countries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd04473-8d5c-488a-8046-95bdfce2e14c",
   "metadata": {},
   "source": [
    "- Iterate over the matches and create a `Span` with the label `\"GPE\"` (geopolitical entity).\n",
    "- Overwrite the entities in `doc.ents` and add the matched span.\n",
    "- Get the matched span‚Äôs root head token.\n",
    "- Print the text of the head token and the span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38952708-56df-444a-8c71-f3ad855ab739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in --> Namibia\n",
      "in --> South Africa\n",
      "elections --> Cambodia\n",
      "of --> Kuwait\n",
      "as --> Somalia\n",
      "Somalia --> Haiti\n",
      "Haiti --> Mozambique\n",
      "in --> Somalia\n",
      "for --> Rwanda\n",
      "Britain --> Singapore\n",
      "War --> Sierra Leone\n",
      "of --> Afghanistan\n",
      "invaded --> Iraq\n",
      "in --> Sudan\n",
      "of --> Congo\n",
      "earthquake --> Haiti\n",
      "[('Namibia', 'GPE'), ('South Africa', 'GPE'), ('Cambodia', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Somalia', 'GPE'), ('Rwanda', 'GPE'), ('Singapore', 'GPE'), ('Sierra Leone', 'GPE'), ('Afghanistan', 'GPE'), ('Iraq', 'GPE'), ('Sudan', 'GPE'), ('Congo', 'GPE'), ('Haiti', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "with open(\"../../exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"../../exercises/en/country_text.txt\", encoding=\"utf8\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Create a doc and reset existing entities\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2997c5-71d8-48a9-8aa6-7b4f128348d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
