{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709b23fe-dd20-4cb8-9d76-96779cf6fc5f",
   "metadata": {},
   "source": [
    "This notebooks is created using Chapter 3 of the the [Advanced NLP with spaCy](https://course.spacy.io/en/chapter3) course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c7626-2299-4595-9c3e-21c79312dcba",
   "metadata": {},
   "source": [
    "# Chapter 3: Processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30c0b1-d949-4d2a-8105-b8a9de99be1b",
   "metadata": {},
   "source": [
    "## Processing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee58301-cf7d-48b7-b617-50775e42a93f",
   "metadata": {},
   "source": [
    "Welcome back! This chapter is dedicated to processing pipelines: a series of functions applied to a doc to add attributes like part-of-speech tags, dependency labels or named entities.\n",
    "\n",
    "In this lesson, you'll learn about the pipeline components provided by spaCy, and what happens behind the scenes when you call nlp on a string of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c02fb5-1416-4ff8-a1b1-53fdf03cd3de",
   "metadata": {},
   "source": [
    "### What happens when you call nlp?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebc5ba3-a464-40b3-8d8e-4f6c0bb76c84",
   "metadata": {},
   "source": [
    "You've already written this plenty of times by now: pass a string of text to the `nlp` object, and receive a `Doc` object.\n",
    "\n",
    "But what does the `nlp` object actually do?\n",
    "\n",
    "First, the tokenizer is applied to turn the string of text into a `Doc` object. Next, a series of pipeline components is applied to the doc in order. In this case, the tagger, then the parser, then the entity recognizer. Finally, the processed doc is returned, so you can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3765a415-5ade-42d0-a908-4652f5d76567",
   "metadata": {},
   "source": [
    "![pipeline](./img/pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893445e1-c733-48a1-ac9d-e2f38b92a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0af013a9-fa54-4b4a-90ff-37d7e24bd30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is a sentence."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e20007-9d17-42cc-ad45-76b55584720c",
   "metadata": {},
   "source": [
    "### Built-in pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8acc4-313b-413d-a628-dfaced5509b9",
   "metadata": {},
   "source": [
    "spaCy ships with a variety of built-in pipeline components. Here are some of the most common ones that you'll want to use in your projects.\n",
    "\n",
    "The part-of-speech tagger sets the `token.tag` and `token.pos` attributes.\n",
    "\n",
    "The dependency parser adds the `token.dep` and `token.head` attributes and is also responsible for detecting sentences and base noun phrases, also known as noun chunks.\n",
    "\n",
    "The named entity recognizer adds the detected entities to the `doc.ents` property. It also sets entity type attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "\n",
    "Finally, the text classifier sets category labels that apply to the whole text, and adds them to the `doc.cats` property.\n",
    "\n",
    "Because text categories are always very specific, the text classifier is not included in any of the trained pipelines by default. But you can use it to train your own system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5019a99-40f8-42af-81af-139b0759cb40",
   "metadata": {},
   "source": [
    "|Name | Description | Creates\n",
    "------|------------|-------\n",
    "tagger | Part-of-speech tagger | Token.tag, Token.pos\n",
    "parser | Dependency parser | Token.dep, Token.head, Doc.sents, Doc.noun_chunks\n",
    "ner\t| Named entity recognizer | Doc.ents, Token.ent_iob, Token.ent_type\n",
    "textcat\t| Text classifier | Doc.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1127bf-ae93-4833-b433-6da59d6b8e8a",
   "metadata": {},
   "source": [
    "### Under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa1e21-1cca-43ae-a828-ddf973256000",
   "metadata": {},
   "source": [
    "All pipeline packages you can load into spaCy include several files and a `config.cfg`.\n",
    "\n",
    "The config defines things like the language and pipeline. This tells spaCy which components to instantiate and how they should be configured.\n",
    "\n",
    "The built-in components that make predictions also need binary data. The data is included in the pipeline package and loaded into the component when you load the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e095d-7d5d-419a-86ab-6e7ae80d49bf",
   "metadata": {},
   "source": [
    "![Package Meta](./img/package_meta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c2099-5352-4d17-b717-2bf4bb396797",
   "metadata": {},
   "source": [
    "- Pipeline defined in model's `config.cfg` in order\n",
    "- Built-in components need binary data to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cc960-f46c-4b58-8ec8-76e55c9c0ca3",
   "metadata": {},
   "source": [
    "### Pipeline attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422d45d-2394-4521-8d60-fbd18328dd29",
   "metadata": {},
   "source": [
    "To see the names of the pipeline components present in the current nlp object, you can use the `nlp.pipe_names` attribute.\n",
    "\n",
    "For a list of component name and component function tuples, you can use the `nlp.pipeline` attribute.\n",
    "\n",
    "The component functions are the functions applied to the doc to process it and set attributes – for example, part-of-speech tags or named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed7f46-232d-4549-aa03-42267af0538e",
   "metadata": {},
   "source": [
    "- `nlp.pipe_names`: list of pipeline component names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a7b406-377e-435f-a399-a5ac7a9355f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d208322-3ecc-4dc2-90c9-7fd1ca5855b1",
   "metadata": {},
   "source": [
    "- nlp.pipeline: list of (name, component) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "098fc93d-076b-4397-9afe-bf3d6445266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7d470a3e5430>)\n",
      "('tagger', <spacy.pipeline.tagger.Tagger object at 0x7d470a3e72f0>)\n",
      "('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7d470a217df0>)\n",
      "('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7d47071a85d0>)\n",
      "('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7d470a44ca10>)\n",
      "('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7d470a217ca0>)\n"
     ]
    }
   ],
   "source": [
    "for x in nlp.pipeline:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e68641-a02a-4881-920f-f0f941b40ca0",
   "metadata": {},
   "source": [
    "## What happens with you call nlp?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76862f-8c5e-412d-a9c9-2848f1c47e8f",
   "metadata": {},
   "source": [
    "What does spaCy do when you call nlp on a string of text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57a2290d-5bd9-46a6-a882-022cecbe4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7653de82-5edf-466b-8034-2196a22a5dca",
   "metadata": {},
   "source": [
    "The tokenizer turns a string of text into a Doc object. spaCy then applies every component in the pipeline on document, in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a105d-e557-47a7-a911-ab4d62845dac",
   "metadata": {},
   "source": [
    "## Inspecting the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5461a1-8a10-4f65-a2a4-c614c235b0ed",
   "metadata": {},
   "source": [
    "Let’s inspect the small English pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae051337-8c74-4634-a992-61ba3c3f3309",
   "metadata": {},
   "source": [
    "- Load the `en_core_web_sm` pipeline and create the `nlp` object.\n",
    "- Print the names of the pipeline components using `nlp.pipe_names`.\n",
    "- Print the full pipeline of `(name, component)` tuples using `nlp.pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "219811db-c089-41ca-9a4c-de33ab14af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7d470a3e5430>)\n",
      "('tagger', <spacy.pipeline.tagger.Tagger object at 0x7d470a3e72f0>)\n",
      "('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7d470a217df0>)\n",
      "('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7d47071a85d0>)\n",
      "('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7d470a44ca10>)\n",
      "('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7d470a217ca0>)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "for x in nlp.pipeline:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc455f54-9b6f-4ddc-a1c7-14fbd5620b01",
   "metadata": {},
   "source": [
    "## Custom pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de111e2-2395-4df7-bf76-42bb884bb7f7",
   "metadata": {},
   "source": [
    "### Why custom components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5b8ec8-0b32-4e00-89c4-d50fab3e94f4",
   "metadata": {},
   "source": [
    "After the text is tokenized and a `Doc` object has been created, pipeline components are applied in order. spaCy supports a range of built-in components, but also lets you define your own.\n",
    "\n",
    "Custom components are executed automatically when you call the `nlp` object on a text.\n",
    "\n",
    "They're especially useful for adding your own custom metadata to documents and tokens.\n",
    "\n",
    "You can also use them to update built-in attributes, like the named entity spans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec8075-3836-442d-ab2d-95a0924e069a",
   "metadata": {},
   "source": [
    "![pipeline](./img/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0188f-661a-405c-8e8d-d06d6e35499c",
   "metadata": {},
   "source": [
    "- Make a function execute automatically when you call `nlp`\n",
    "- Add your own metadata to documents and tokens\n",
    "- Updating built-in attributes like `doc.ents`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e135087-2963-44ca-8d10-b3a15a45e038",
   "metadata": {},
   "source": [
    "### Anatomy of a component(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba781d8-0813-4a52-8f21-3799064dc60c",
   "metadata": {},
   "source": [
    "Fundamentally, a pipeline component is a function or callable that takes a doc, modifies it and returns it, so it can be processed by the next component in the pipeline.\n",
    "\n",
    "To tell spaCy where to find your custom component and how it should be called, you can decorate it using the `@Language.component` decorator. Just add it to the line right above the function definition.\n",
    "\n",
    "Once a component is registered, it can be added to the pipeline using the `nlp.add_pipe` method. The method takes at least one argument: the string name of the component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00654f3e-0d33-4d66-87ef-ebacf52f38e4",
   "metadata": {},
   "source": [
    "- Function that takes a `doc`, modifies it and returns it\n",
    "- Registered using the `Language.component` decorator\n",
    "- Can be added using the `nlp.add_pipe` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b71756c8-86e7-473f-a42e-3f33fd232118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.custom_component_function(doc)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Do something to the doc here\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"custom_component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711fa48-f457-44ed-9cc9-31fec241d771",
   "metadata": {},
   "source": [
    "### Anatomy of a component(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a41fd6-ca2a-48c1-98ce-fe196a8c6fb5",
   "metadata": {},
   "source": [
    "To specify *where* to add the component in the pipeline, you can use the following keyword arguments:\n",
    "\n",
    "Setting `last` to `True` will add the component last in the pipeline. This is the default behavior.\n",
    "\n",
    "Setting `first` to `True` will add the component first in the pipeline, right after the tokenizer.\n",
    "\n",
    "The `before` and `after` arguments let you define the name of an existing component to add the new component before or after. For example, `before=\"ner\"` will add it before the named entity recognizer.\n",
    "\n",
    "The other component to add the new component before or after needs to exist, though – otherwise, spaCy will raise an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87da9b13-31ac-4d80-a6bd-91dde9faad36",
   "metadata": {},
   "source": [
    "Argument | Description | Example\n",
    "-----------|-----------|--------\n",
    "last | If True, add last | nlp.add_pipe(\"component\", last=True)\n",
    "first | If True, add first | nlp.add_pipe(\"component\", first=True)\n",
    "before | Add before component | nlp.add_pipe(\"component\", before=\"ner\")\n",
    "after | Add after component | nlp.add_pipe(\"component\", after=\"tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ce1df-eef5-4036-9c69-63c5a00a1cae",
   "metadata": {},
   "source": [
    "### Example: a simple component(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89498c-84cf-4622-8a30-d60cee376e27",
   "metadata": {},
   "source": [
    "Here's an example of a simple pipeline component.\n",
    "\n",
    "We start off with the small English pipeline.\n",
    "\n",
    "We then define the component – a function that takes a `Doc` object and returns it.\n",
    "\n",
    "Let's do something simple and print the length of the doc that passes through the pipeline.\n",
    "\n",
    "Don't forget to return the doc so it can be processed by the next component in the pipeline! The doc created by the tokenizer is passed through all components, so it's important that they all return the modified doc.\n",
    "\n",
    "To tell spaCy about the new component, we register it using the `@Language.component` decorator and call it \"custom_component\".\n",
    "\n",
    "We can now add the component to the pipeline. Let's add it to the very beginning right after the tokenizer by setting `first=True`.\n",
    "\n",
    "When we print the pipeline component names, the custom component now shows up at the start. This means it will be applied first when we process a doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bf35847-2b7a-4d3b-8610-e2f87dd06e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08e25c-a2f8-4a47-8a1b-df9c025ba2e3",
   "metadata": {},
   "source": [
    "### Example: a simple component(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16bd062-5d76-4db2-99cd-4fb6a85cab95",
   "metadata": {},
   "source": [
    "Now when we process a text using the nlp object, the custom component will be applied to the doc and the length of the document will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b9370e-399e-4bcc-8102-c059bc2617d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363c5c6-bc51-45eb-9a27-1b6f5c500a01",
   "metadata": {},
   "source": [
    "## Use cases for custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920aa88d-117e-4c39-b66e-8e58ed23f327",
   "metadata": {},
   "source": [
    "Which of these problems can be solved by custom pipeline components? Choose all that apply!\n",
    "\n",
    "1. Updating the trained pipelines and improving their predictions\n",
    "2. Computing your own values based on tokens and their attributes\n",
    "3. Adding named entities, for example based on a dictionary\n",
    "4. Implementing support for an additional language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04180a22-ef53-4b60-ab1c-6e902e1f29bf",
   "metadata": {},
   "source": [
    "Answer: 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e71a4-4271-41c0-9481-680c6fbeba24",
   "metadata": {},
   "source": [
    "Custom components are great for adding custom values to documents, tokens and spans, and customizing the `doc.ents`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0272c6-0bc4-4965-9d3d-5adac72fde64",
   "metadata": {},
   "source": [
    "## Simple components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8f1dc-9c7d-499d-a8bb-e7f92ee8c358",
   "metadata": {},
   "source": [
    "The example shows a custom component that prints the number of tokens in a document. Can you complete it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096c838-c1b5-44fd-9c78-a219659d530a",
   "metadata": {},
   "source": [
    "- Complete the component function with the `doc`’s length.\n",
    "- Add the `\"length_component\"` to the existing pipeline as the **first** component.\n",
    "- Try out the new pipeline and process any text with the `nlp` object – for example “This is a sentence.”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34122230-313e-4b1d-92e4-9b3befec643a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "This document is 4 tokens long.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"length_component\")\n",
    "def length_component_function(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3c0e4-cfa9-4329-bcb9-373a10134368",
   "metadata": {},
   "source": [
    "## Complex components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69437cd2-5ee9-4267-81ff-bc1cfb557a46",
   "metadata": {},
   "source": [
    "In this exercise, you’ll be writing a custom component that uses the `PhraseMatcher` to find animal names in the document and adds the matched spans to the `doc.ents`. A `PhraseMatcher` with the animal patterns has already been created as the variable `matcher`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa10d38-2c3c-4237-91b4-2eb43a43c9f3",
   "metadata": {},
   "source": [
    "- Define the custom component and apply the `matcher` to the `doc`.\n",
    "- Create a `Span` for each match, assign the label ID for `\"ANIMAL\"` and overwrite the `doc.ents` with the new spans.\n",
    "- Add the new component to the pipeline *after* the `\"ner\"` component.\n",
    "- Process the text and print the entity text and entity label for the entities in `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05734a2-24ce-4d8b-94b1-3600a8d61421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'animal_component', 'ner']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", before=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1410462-1df0-4a2d-b321-e125afe7f4f4",
   "metadata": {},
   "source": [
    "## Extension attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095d575-e4ae-4a78-a4e6-1245d91c8b46",
   "metadata": {},
   "source": [
    "In this lesson, you'll learn how to add custom attributes to the `Doc`, `Token` and `Span` objects to store custom data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a5aea-f5dc-443b-beb1-187624139be3",
   "metadata": {},
   "source": [
    "### Setting custom attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad0c03-6626-4f0c-b57b-afa5e8bcf889",
   "metadata": {},
   "source": [
    "Custom attributes let you add any metadata to docs, tokens and spans. The data can be added once, or it can be computed dynamically.\n",
    "\n",
    "Custom attributes are available via the `._` (dot underscore) property. This makes it clear that they were added by the user, and not built into spaCy, like token.text.\n",
    "\n",
    "Attributes need to be registered on the global `Doc`, `Token` and `Span` classes you can import from `spacy.tokens`. You've already worked with those in the previous chapters. To register a custom attribute on the `Doc`, `Token` and `Span`, you can use the `set_extension` method.\n",
    "\n",
    "The first argument is the attribute name. Keyword arguments let you define how the value should be computed. In this case, it has a default value and can be overwritten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c996cf24-380c-4e47-82a3-96da31ad37c1",
   "metadata": {},
   "source": [
    "- Add custom metadata to documents, tokens and spans\n",
    "- Accessible via the `._` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59bb0320-4d74-4d58-a155-871cf7c60b71",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E047] Can't assign a value to unregistered extension attribute 'title'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m doc\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mtitle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy document\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m token\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mis_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m span\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mhas_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-spacy/lib/python3.12/site-packages/spacy/tokens/underscore.py:76\u001b[0m, in \u001b[0;36mUnderscore.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, value: Any):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE047\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m     77\u001b[0m     default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: [E047] Can't assign a value to unregistered extension attribute 'title'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "doc._.title = \"My document\"\n",
    "token._.is_color = True\n",
    "span._.has_color = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eae57d-ca2b-4ceb-8a2b-4ec962119844",
   "metadata": {},
   "source": [
    "- Registered on the global `Doc`, `Token` or `Span` using the `set_extension` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c66d570-438a-44f0-b8ce-5ec4f12af4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Set extensions on the Doc, Token and Span\n",
    "Doc.set_extension(\"title\", default=None)\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "Span.set_extension(\"has_color\", default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f58e30-8c70-4823-831a-a6beb0fcb8c8",
   "metadata": {},
   "source": [
    "### Extension attribute types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ca5fd-9ea3-4f37-9f8c-a9e2825a8412",
   "metadata": {},
   "source": [
    "- Attribute extensions\n",
    "- Property extensions\n",
    "- Method extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99887fc0-ecc7-4c6c-9006-cabe23eb5944",
   "metadata": {},
   "source": [
    "### Attribute extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718029e-6f98-41bc-8130-3a1226951c71",
   "metadata": {},
   "source": [
    "- Set a default value that can be overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12466d4-933d-46c2-98ac-e1b0b332467f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E090] Extension 'is_color' already exists on Token. To overwrite the existing extension, set `force=True` on `Token.set_extension`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Token\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Set extension on the Token with default value\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m Token\u001b[38;5;241m.\u001b[39mset_extension(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_color\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sky is blue.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Overwrite extension attribute value\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-spacy/lib/python3.12/site-packages/spacy/tokens/token.pyx:65\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.set_extension\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E090] Extension 'is_color' already exists on Token. To overwrite the existing extension, set `force=True` on `Token.set_extension`."
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Set extension on the Token with default value\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "\n",
    "# Overwrite extension attribute value\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f553a9a-8829-4eda-bdbb-0d87d69cbd72",
   "metadata": {},
   "source": [
    "### Property extensions (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4d0cd-12ae-4a85-b6fb-ec10ae88b3a0",
   "metadata": {},
   "source": [
    "Property extensions work like properties in Python: they can define a getter function and an optional setter.\n",
    "\n",
    "The getter function is only called when you retrieve the attribute. This lets you compute the value dynamically, and even take other custom attributes into account.\n",
    "\n",
    "Getter functions take one argument: the object, in this case, the token. In this example, the function returns whether the token text is in our list of colors.\n",
    "\n",
    "We can then provide the function via the `getter` keyword argument when we register the extension.\n",
    "\n",
    "The token \"blue\" now returns `True` for `._.is_color`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87925f72-2b48-445b-be05-62fd48f685ef",
   "metadata": {},
   "source": [
    "- Define a getter and an optional setter function\n",
    "- Getter only called when you retrieve the attribute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7516a69-1ec0-45af-9381-baba8155551a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E090] Extension 'is_color' already exists on Token. To overwrite the existing extension, set `force=True` on `Token.set_extension`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;129;01min\u001b[39;00m colors\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Set extension on the Token with getter\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m Token\u001b[38;5;241m.\u001b[39mset_extension(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_color\u001b[39m\u001b[38;5;124m\"\u001b[39m, getter\u001b[38;5;241m=\u001b[39mget_is_color)\n\u001b[1;32m     11\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sky is blue.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(doc[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mis_color, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, doc[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-spacy/lib/python3.12/site-packages/spacy/tokens/token.pyx:65\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.set_extension\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E090] Extension 'is_color' already exists on Token. To overwrite the existing extension, set `force=True` on `Token.set_extension`."
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return token.text in colors\n",
    "\n",
    "# Set extension on the Token with getter\n",
    "Token.set_extension(\"is_color\", getter=get_is_color)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[3]._.is_color, \"-\", doc[3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948c73c-7c73-4e38-95fb-be0ab27119a3",
   "metadata": {},
   "source": [
    "### Property extensions (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d8c29-ef9e-4c43-9ed0-890868b6fce4",
   "metadata": {},
   "source": [
    "If you want to set extension attributes on a span, you almost always want to use a property extension with a getter. Otherwise, you'd have to update every possible span ever by hand to set all the values.\n",
    "\n",
    "In this example, the `get_has_color` function takes the span and returns whether the text of any of the tokens is in the list of colors.\n",
    "\n",
    "After we've processed the doc, we can check different slices of the doc and the custom `._.has_color` property returns whether the span contains a color token or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b495b-d429-4eee-b9d5-b31c674bff18",
   "metadata": {},
   "source": [
    "- Span extensions should almost always use a getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f26158f5-f7e4-4a5a-bcc3-f0b5cc8b683c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E090] Extension 'has_color' already exists on Span. To overwrite the existing extension, set `force=True` on `Span.set_extension`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;129;01min\u001b[39;00m colors \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m span)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Set extension on the Span with getter\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m Span\u001b[38;5;241m.\u001b[39mset_extension(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_color\u001b[39m\u001b[38;5;124m\"\u001b[39m, getter\u001b[38;5;241m=\u001b[39mget_has_color)\n\u001b[1;32m     11\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sky is blue.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(doc[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mhas_color, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, doc[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-spacy/lib/python3.12/site-packages/spacy/tokens/span.pyx:44\u001b[0m, in \u001b[0;36mspacy.tokens.span.Span.set_extension\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E090] Extension 'has_color' already exists on Span. To overwrite the existing extension, set `force=True` on `Span.set_extension`."
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# Define getter function\n",
    "def get_has_color(span):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Span.set_extension(\"has_color\", getter=get_has_color)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[1:4]._.has_color, \"-\", doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, \"-\", doc[0:2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872da39-a84d-4a27-9350-b25def178d8f",
   "metadata": {},
   "source": [
    "### Method extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856cfcb-7583-42f6-a819-876397c1f602",
   "metadata": {},
   "source": [
    "Method extensions make the extension attribute a callable method.\n",
    "\n",
    "You can then pass one or more arguments to it, and compute attribute values dynamically – for example, based on a certain argument or setting.\n",
    "\n",
    "In this example, the method function checks whether the doc contains a token with a given text. The first argument of the method is always the object itself – in this case, the doc. It's passed in automatically when the method is called. All other function arguments will be arguments on the method extension. In this case, `token_text`.\n",
    "\n",
    "Here, the custom `._.has_token` method returns `True` for the word \"blue\" and `False` for the word \"cloud\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61f367-34b0-4bce-8586-da3fee3e4a18",
   "metadata": {},
   "source": [
    "- Assign a **function** that becomes available as an object method\n",
    "- Lets you pass **arguments** to the extension function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1ca92a8-8d4b-4418-baa5-718a900fafc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n",
      "False - cloud\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token(\"blue\"), \"- blue\")\n",
    "print(doc._.has_token(\"cloud\"), \"- cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294a9c0-e0ea-4ab5-8561-1c3760407a31",
   "metadata": {},
   "source": [
    "## Setting extension attributes(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c31d60c-c3ac-42f1-945f-1f68c9415565",
   "metadata": {},
   "source": [
    "Step 1\n",
    "- Use `Token.set_extension` to register `\"is_country\"` (default `False`).\n",
    "- Update it for `\"Spain\"` and print it for all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9830f144-6558-44ac-a2da-1ea6a2ff3e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f50354-6910-4351-b1db-75396d119d2b",
   "metadata": {},
   "source": [
    "Step 2\n",
    "- Use `Token.set_extension` to register `\"reversed\"` (getter function `get_reversed`).\n",
    "- Print its value for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d61ce643-e17d-43bf-8f0c-f0f71420fef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d28fce-51bf-4320-b685-bd9ec3df3160",
   "metadata": {},
   "source": [
    "## Setting extension attributes (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6839094-ca42-4e9a-a6cc-2d2d9ecda8bf",
   "metadata": {},
   "source": [
    "Let’s try setting some more complex attributes using getters and method extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beaa52a-b22a-440f-a31f-40d34a54ace5",
   "metadata": {},
   "source": [
    "Part 1\n",
    "- Complete the get_has_number function .\n",
    "- Use `Doc.set_extension` to register `\"has_number\"` (getter `get_has_number`) and print its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93d44f0c-81b6-4206-a562-f840fab25cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# Register the Doc property extension \"has_number\" with the getter get_has_number\n",
    "Doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5e6ad2-3cbd-466e-9e5f-8be9be7764b6",
   "metadata": {},
   "source": [
    "Part 2\n",
    "- Use `Span.set_extension` to register `\"to_html\"` (method `to_html`).\n",
    "- Call it on `doc[0:2]` with the tag `\"strong\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a741831d-d5fa-4790-8abb-2efff46e030e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Register the Span method extension \"to_html\" with the method to_html\n",
    "Span.set_extension(\"to_html\", method=to_html,)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43d346-c6c9-4a25-a819-b7fad4a1b29e",
   "metadata": {},
   "source": [
    "## Entities and extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1284960-2e5a-414d-8f30-119c1e10996e",
   "metadata": {},
   "source": [
    "In this exercise, you’ll combine custom extension attributes with the statistical predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d7e3a-577a-4761-91f1-19bed4c8e8a5",
   "metadata": {},
   "source": [
    "- Complete the `get_wikipedia_url` getter so it only returns the URL if the span’s label is in the list of labels.\n",
    "- Set the `Span` extension `\"wikipedia_url\"` using the getter `get_wikipedia_url`.\n",
    "- Iterate over the entities in the `doc` and output their Wikipedia URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92c3c62-c2b4-4782-bb20-2f24d3eb0f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371315cf-102f-44e8-aafd-23f6ac51d759",
   "metadata": {},
   "source": [
    "## Components with extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8e091-2343-44bc-95b0-b2f820dfbffa",
   "metadata": {},
   "source": [
    "Extension attributes are especially powerful if they’re combined with custom pipeline components. In this exercise, you’ll write a pipeline component that finds country names and a custom extension attribute that returns a country’s capital, if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e46d2-0513-4e82-9e5e-220f5e5f4cf7",
   "metadata": {},
   "source": [
    "A phrase matcher with all countries is available as the variable `matcher`. A dictionary of countries mapped to their capital cities is available as the variable `CAPITALS`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fdef2b-ad69-4a74-a4c7-055da2bc2547",
   "metadata": {},
   "source": [
    "- Complete the `countries_component_function` and create a `Span` with the label `\"GPE\"` (geopolitical entity) for all matches.\n",
    "- Add the component to the pipeline.\n",
    "- Register the Span extension attribute `\"capital\"` with the getter `get_capital`.\n",
    "- Process the text and print the entity text, entity label and entity capital for each entity span in `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edef4739-fa45-4bb6-a5ed-7f1aad3b0923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_component']\n",
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"../../exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"../../exercises/en/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component_function(doc):\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d67b8-e150-4633-bb34-7585ed2a6b55",
   "metadata": {},
   "source": [
    "## Scaling and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f3cee-3838-4dc6-83ec-f5dda3907feb",
   "metadata": {},
   "source": [
    "In this lesson, I'll show you a few tips and tricks to make your spaCy pipelines run as fast as possible, and process large volumes of text efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e317736-f335-4baa-ad9a-17cf9cc382ce",
   "metadata": {},
   "source": [
    "### Processing large volumes of text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0dfc1d-3bb0-426c-9ffb-89750a35d145",
   "metadata": {},
   "source": [
    "If you need to process a lot of texts and create a lot of `Doc` objects in a row, the `nlp.pipe` method can speed this up significantly.\n",
    "\n",
    "It processes the texts as a stream and yields `Doc` objects.\n",
    "\n",
    "It is much faster than just calling nlp on each text, because it batches up the texts.\n",
    "\n",
    "`nlp.pipe` is a generator that yields `Doc` objects, so in order to get a list of docs, remember to call the `list` method around it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b6db4-4d28-4591-b006-72380b0ae849",
   "metadata": {},
   "source": [
    "- Use `nlp.pipe` method\n",
    "- Processes texts as a stream, yields `Doc` objects\n",
    "- Much faster than calling `nlp` on each text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ffb4e8-bc6b-437d-82b6-989b159f7e80",
   "metadata": {},
   "source": [
    "**BAD:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "343732e2-9d94-4246-8969-8e8a5c8be998",
   "metadata": {},
   "source": [
    "docs = [nlp(text) for text in LOTS_OF_TEXTS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa0d34-89ff-4505-9aef-b13895d45b15",
   "metadata": {},
   "source": [
    "**GOOD:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6334d400-f157-464a-8c9e-1ddfcd77adfc",
   "metadata": {},
   "source": [
    "docs = list(nlp.pipe(LOTS_OF_TEXTS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de983e9-4613-4978-a2aa-c61c43ffc354",
   "metadata": {},
   "source": [
    "### Passing in context (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314cdff-9f14-4001-8692-29d5a473b423",
   "metadata": {},
   "source": [
    "`nlp.pipe` also supports passing in tuples of text / context if you set `as_tuples` to `True`.\n",
    "\n",
    "The method will then yield doc / context tuples.\n",
    "\n",
    "This is useful for passing in additional metadata, like an ID associated with the text, or a page number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491105b-c7bc-4d9c-b457-3ad8674ee061",
   "metadata": {},
   "source": [
    "- Setting `as_tuples=True` on `nlp.pipe` lets you pass in `(text, context)` tuples\n",
    "- Yields `(doc, context)` tuples\n",
    "- Useful for associating metadata with the `doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "040f4a6b-f15b-4bec-b0b3-44b7cf0ce3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"page_number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f7262-2f86-481a-a293-59d57496e46d",
   "metadata": {},
   "source": [
    "### Passing in context (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059a095-e3e2-402e-bc49-ccba6e59de45",
   "metadata": {},
   "source": [
    "You can even add the context metadata to custom attributes.\n",
    "\n",
    "In this example, we're registering two extensions, `id` and `page_number`, which default to `None`.\n",
    "\n",
    "After processing the text and passing through the context, we can overwrite the doc extensions with our context metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625588cc-6213-47ae-9824-326c681e9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"id\", default=None)\n",
    "Doc.set_extension(\"page_number\", default=None)\n",
    "\n",
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context[\"id\"]\n",
    "    doc._.page_number = context[\"page_number\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15962e9-5f0a-4dde-8c59-3a0f8a79b4b5",
   "metadata": {},
   "source": [
    "### Using only the tokenizer (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17eae0-b036-43be-aaf2-51378a6db85c",
   "metadata": {},
   "source": [
    "Another common scenario: Sometimes you already have a model loaded to do other processing, but you only need the tokenizer for one particular text.\n",
    "\n",
    "Running the whole pipeline is unnecessarily slow, because you'll be getting a bunch of predictions from the model that you don't need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a7111d-2577-4483-bbd8-7937448f6fc2",
   "metadata": {},
   "source": [
    "![pipeline](./img/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2a5e4-c9e1-4bfb-903c-dfd1438aca69",
   "metadata": {},
   "source": [
    "- don't run the whole pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524faafa-d812-4a8d-a449-0d0c13c668be",
   "metadata": {},
   "source": [
    "### Using only the tokenizer (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046707d-8265-40ef-8d8e-b6e8fad7407d",
   "metadata": {},
   "source": [
    "If you only need a tokenized `Doc` object, you can use the `nlp.make_doc` method instead, which takes a text and returns a doc.\n",
    "\n",
    "This is also how spaCy does it behind the scenes: `nlp.make_doc` turns the text into a doc before the pipeline components are called."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a1f3fa-395e-4ff4-a6f7-6b6f460682d6",
   "metadata": {},
   "source": [
    "- Use `nlp.make_doc` to turn a text into a `Doc` object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb859c-f3ad-4ea9-a391-d8cdf8ca2e5c",
   "metadata": {},
   "source": [
    "**BAD:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43662b83-fcbd-4fc4-86b9-ee92d992e08a",
   "metadata": {},
   "source": [
    "doc = nlp(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4cb7ee-00f9-46c6-a16b-34182769510d",
   "metadata": {},
   "source": [
    "**GOOD:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eea1c64f-f667-46f2-8af1-bfc5d0d2fcdb",
   "metadata": {},
   "source": [
    "doc = nlp.make_doc(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42caae6-2046-4dba-a74c-83d3c7b544af",
   "metadata": {},
   "source": [
    "### Disabling pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6eb98e-d389-4065-af7f-fc33f7403e87",
   "metadata": {},
   "source": [
    "spaCy also allows you to temporarily disable pipeline components using the `nlp.select_pipes` context manager.\n",
    "\n",
    "It accepts the keyword arguments `enable` or `disable` that can define a list of string names of the pipeline components to disable. For example, if you only want to use the entity recognizer to process a document, you can temporarily disable the tagger and parser.\n",
    "\n",
    "After the with `block`, the disabled pipeline components are automatically restored.\n",
    "\n",
    "In the `with` block, spaCy will only run the remaining components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d3b67-4de1-4849-b7b6-6390118e6f03",
   "metadata": {},
   "source": [
    "- Use `nlp.select_pipes` to temporarily disable one or more pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c175bb70-8bd1-44cf-8694-a947483cca83",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E001] No component 'tagger' found in pipeline. Available names: ['countries_component']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Disable tagger and parser\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mselect_pipes(disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Process the text and print the entities\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(doc\u001b[38;5;241m.\u001b[39ments)\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-spacy/lib/python3.12/site-packages/spacy/language.py:1111\u001b[0m, in \u001b[0;36mLanguage.select_pipes\u001b[0;34m(self, disable, enable)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;66;03m# DisabledPipes will restore the pipes in 'disable' when it's done, so we need to exclude\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;66;03m# those pipes that were already disabled.\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m disable \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m disable \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disabled]\n\u001b[0;32m-> 1111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DisabledPipes(\u001b[38;5;28mself\u001b[39m, disable)\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-spacy/lib/python3.12/site-packages/spacy/language.py:2345\u001b[0m, in \u001b[0;36mDisabledPipes.__init__\u001b[0;34m(self, nlp, names)\u001b[0m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m names\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames:\n\u001b[0;32m-> 2345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp\u001b[38;5;241m.\u001b[39mdisable_pipe(name)\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames)\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-spacy/lib/python3.12/site-packages/spacy/language.py:1005\u001b[0m, in \u001b[0;36mLanguage.disable_pipe\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Disable a pipeline component. The component will still exist on\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;124;03mthe nlp object, but it won't be run as part of the pipeline. Does\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;124;03mnothing if the component is already disabled.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \n\u001b[1;32m   1002\u001b[0m \u001b[38;5;124;03mname (str): The name of the component to disable.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE001\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, opts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names))\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disabled\u001b[38;5;241m.\u001b[39madd(name)\n",
      "\u001b[0;31mValueError\u001b[0m: [E001] No component 'tagger' found in pipeline. Available names: ['countries_component']"
     ]
    }
   ],
   "source": [
    "# Disable tagger and parser\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"parser\"]):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be062b48-f8af-41a4-acae-dc2ae2fcac96",
   "metadata": {},
   "source": [
    "- Restores them after the with block\n",
    "- Only runs the remaining components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21914ca-968a-4030-8adf-a3ffc16f3d70",
   "metadata": {},
   "source": [
    "## Processing streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72179ba-3e20-489f-b2fa-500a6fd8aab6",
   "metadata": {},
   "source": [
    "In this exercise, you’ll be using `nlp.pipe` for more efficient text processing. The `nlp` object has already been created for you. A list of tweets about a popular American fast food chain are available as the variable `TEXTS`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e3ee7-7d0f-4de4-b08c-3efe5dfe352a",
   "metadata": {},
   "source": [
    "Part 1\n",
    "\n",
    "- Rewrite the example to use `nlp.pipe`. Instead of iterating over the texts and processing them, iterate over the `doc` objects yielded by `nlp.pipe`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03caccb-9a67-4900-966e-20b656e26f3a",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad2b9735-6b8b-44a2-b9cd-c6c2fdde279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['SANDWICH']\n",
      "['terrible', 'gettin']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"../../exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "for text in TEXTS:\n",
    "    doc = nlp(text)\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8abcc-eccb-4f99-88f4-a4f3e236bd5c",
   "metadata": {},
   "source": [
    "Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27b58995-bafc-43a4-9539-14353a941014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['SANDWICH']\n",
      "['terrible', 'gettin']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"../../exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e1269-2482-41a2-905a-cb50e1e43dcd",
   "metadata": {},
   "source": [
    "Part 2\n",
    "- Rewrite the example to use `nlp.pipe`. Don’t forget to call `list()` around the result to turn it into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf24d4a-5d13-4f1f-85a5-4c3ec7c892e2",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bd839b5-0d3d-46a1-94e5-ff8a36cad88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) () (McDonalds,) (McDonalds, Spain) (The Arch Deluxe :P,) () (This morning,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"../../exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = [nlp(text) for text in TEXTS]\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f079c-6c94-40de-aaa3-569f6c9dc9c2",
   "metadata": {},
   "source": [
    "Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6c0bd0b-6fcd-4daf-a2ab-fed796e3a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) () (McDonalds,) (McDonalds, Spain) (The Arch Deluxe :P,) () (This morning,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"../../exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146276c7-a709-40d5-84b4-3e9dfbc509f1",
   "metadata": {},
   "source": [
    "Part 3\n",
    "- Rewrite the example to use `nlp.pipe`. Don’t forget to call `list()` around the result to turn it into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba00cd-c9a6-4bde-b112-c05d41040f4a",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81c76e14-5873-4e2f-b496-862e97985be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = [nlp(person) for person in people]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f026b10-27a0-4598-9235-40cd86698489",
   "metadata": {},
   "source": [
    "Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e5f66a6-87b0-410b-8e93-3b3e54c655d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = list(nlp.pipe(people))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e9b308-8fa5-4d71-a01c-b09d57befee6",
   "metadata": {},
   "source": [
    "## Processing data with context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e304e6-d347-40d1-bb91-654a08f0b016",
   "metadata": {},
   "source": [
    "In this exercise, you’ll be using custom attributes to add author and book meta information to quotes.\n",
    "\n",
    "A list of `[text, context]` examples is available as the variable `DATA`. The texts are quotes from famous books, and the contexts dictionaries with the keys `\"author\"` and `\"book\"`.\n",
    "\n",
    "- Use the `set_extension` method to register the custom attributes `\"author\"` and `\"book\"` on the `Doc`, which default to `None`.\n",
    "- Process the `[text, context]` pairs in `DATA` using `nlp.pipe` with `as_tuples=True`.\n",
    "- Overwrite the `doc._.book` and `doc._.author` with the respective info passed in as the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19aff1d0-0d77-4415-9cd7-dddc97eae6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\n",
      " — 'Metamorphosis' by Franz Kafka\n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing.\n",
      " — 'Moby-Dick or, The Whale' by Herman Melville\n",
      "\n",
      "It was the best of times, it was the worst of times.\n",
      " — 'A Tale of Two Cities' by Charles Dickens\n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\n",
      " — 'On the Road' by Jack Kerouac\n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      " — '1984' by George Orwell\n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing.\n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "with open(\"../../exercises/en/bookquotes.json\", encoding=\"utf8\") as f:\n",
    "    DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Register the Doc extension \"author\" (default None)\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "\n",
    "# Register the Doc extension \"book\" (default None)\n",
    "Doc.set_extension(\"book\", default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(f\"{doc.text}\\n — '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f0f62-5419-43b1-81d3-5a8ac923996f",
   "metadata": {},
   "source": [
    "## Selective processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975adb35-2de6-4569-9f34-0192541fd048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
