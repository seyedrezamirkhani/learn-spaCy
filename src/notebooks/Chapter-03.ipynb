{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709b23fe-dd20-4cb8-9d76-96779cf6fc5f",
   "metadata": {},
   "source": [
    "This notebooks is created using Chapter 3 of the the [Advanced NLP with spaCy](https://course.spacy.io/en/chapter3) course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c7626-2299-4595-9c3e-21c79312dcba",
   "metadata": {},
   "source": [
    "# Chapter 3: Processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30c0b1-d949-4d2a-8105-b8a9de99be1b",
   "metadata": {},
   "source": [
    "## Processing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee58301-cf7d-48b7-b617-50775e42a93f",
   "metadata": {},
   "source": [
    "Welcome back! This chapter is dedicated to processing pipelines: a series of functions applied to a doc to add attributes like part-of-speech tags, dependency labels or named entities.\n",
    "\n",
    "In this lesson, you'll learn about the pipeline components provided by spaCy, and what happens behind the scenes when you call nlp on a string of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c02fb5-1416-4ff8-a1b1-53fdf03cd3de",
   "metadata": {},
   "source": [
    "### What happens when you call nlp?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebc5ba3-a464-40b3-8d8e-4f6c0bb76c84",
   "metadata": {},
   "source": [
    "You've already written this plenty of times by now: pass a string of text to the `nlp` object, and receive a `Doc` object.\n",
    "\n",
    "But what does the `nlp` object actually do?\n",
    "\n",
    "First, the tokenizer is applied to turn the string of text into a `Doc` object. Next, a series of pipeline components is applied to the doc in order. In this case, the tagger, then the parser, then the entity recognizer. Finally, the processed doc is returned, so you can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3765a415-5ade-42d0-a908-4652f5d76567",
   "metadata": {},
   "source": [
    "![pipeline](./img/pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893445e1-c733-48a1-ac9d-e2f38b92a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0af013a9-fa54-4b4a-90ff-37d7e24bd30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is a sentence."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e20007-9d17-42cc-ad45-76b55584720c",
   "metadata": {},
   "source": [
    "### Built-in pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8acc4-313b-413d-a628-dfaced5509b9",
   "metadata": {},
   "source": [
    "spaCy ships with a variety of built-in pipeline components. Here are some of the most common ones that you'll want to use in your projects.\n",
    "\n",
    "The part-of-speech tagger sets the `token.tag` and `token.pos` attributes.\n",
    "\n",
    "The dependency parser adds the `token.dep` and `token.head` attributes and is also responsible for detecting sentences and base noun phrases, also known as noun chunks.\n",
    "\n",
    "The named entity recognizer adds the detected entities to the `doc.ents` property. It also sets entity type attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "\n",
    "Finally, the text classifier sets category labels that apply to the whole text, and adds them to the `doc.cats` property.\n",
    "\n",
    "Because text categories are always very specific, the text classifier is not included in any of the trained pipelines by default. But you can use it to train your own system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5019a99-40f8-42af-81af-139b0759cb40",
   "metadata": {},
   "source": [
    "|Name | Description | Creates\n",
    "------|------------|-------\n",
    "tagger | Part-of-speech tagger | Token.tag, Token.pos\n",
    "parser | Dependency parser | Token.dep, Token.head, Doc.sents, Doc.noun_chunks\n",
    "ner\t| Named entity recognizer | Doc.ents, Token.ent_iob, Token.ent_type\n",
    "textcat\t| Text classifier | Doc.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1127bf-ae93-4833-b433-6da59d6b8e8a",
   "metadata": {},
   "source": [
    "### Under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa1e21-1cca-43ae-a828-ddf973256000",
   "metadata": {},
   "source": [
    "All pipeline packages you can load into spaCy include several files and a `config.cfg`.\n",
    "\n",
    "The config defines things like the language and pipeline. This tells spaCy which components to instantiate and how they should be configured.\n",
    "\n",
    "The built-in components that make predictions also need binary data. The data is included in the pipeline package and loaded into the component when you load the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e095d-7d5d-419a-86ab-6e7ae80d49bf",
   "metadata": {},
   "source": [
    "![Package Meta](./img/package_meta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c2099-5352-4d17-b717-2bf4bb396797",
   "metadata": {},
   "source": [
    "- Pipeline defined in model's `config.cfg` in order\n",
    "- Built-in components need binary data to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cc960-f46c-4b58-8ec8-76e55c9c0ca3",
   "metadata": {},
   "source": [
    "### Pipeline attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422d45d-2394-4521-8d60-fbd18328dd29",
   "metadata": {},
   "source": [
    "To see the names of the pipeline components present in the current nlp object, you can use the `nlp.pipe_names` attribute.\n",
    "\n",
    "For a list of component name and component function tuples, you can use the `nlp.pipeline` attribute.\n",
    "\n",
    "The component functions are the functions applied to the doc to process it and set attributes – for example, part-of-speech tags or named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed7f46-232d-4549-aa03-42267af0538e",
   "metadata": {},
   "source": [
    "- `nlp.pipe_names`: list of pipeline component names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a7b406-377e-435f-a399-a5ac7a9355f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d208322-3ecc-4dc2-90c9-7fd1ca5855b1",
   "metadata": {},
   "source": [
    "- nlp.pipeline: list of (name, component) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "098fc93d-076b-4397-9afe-bf3d6445266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7d470a3e5430>)\n",
      "('tagger', <spacy.pipeline.tagger.Tagger object at 0x7d470a3e72f0>)\n",
      "('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7d470a217df0>)\n",
      "('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7d47071a85d0>)\n",
      "('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7d470a44ca10>)\n",
      "('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7d470a217ca0>)\n"
     ]
    }
   ],
   "source": [
    "for x in nlp.pipeline:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e68641-a02a-4881-920f-f0f941b40ca0",
   "metadata": {},
   "source": [
    "## What happens with you call nlp?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76862f-8c5e-412d-a9c9-2848f1c47e8f",
   "metadata": {},
   "source": [
    "What does spaCy do when you call nlp on a string of text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57a2290d-5bd9-46a6-a882-022cecbe4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7653de82-5edf-466b-8034-2196a22a5dca",
   "metadata": {},
   "source": [
    "The tokenizer turns a string of text into a Doc object. spaCy then applies every component in the pipeline on document, in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a105d-e557-47a7-a911-ab4d62845dac",
   "metadata": {},
   "source": [
    "## Inspecting the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5461a1-8a10-4f65-a2a4-c614c235b0ed",
   "metadata": {},
   "source": [
    "Let’s inspect the small English pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae051337-8c74-4634-a992-61ba3c3f3309",
   "metadata": {},
   "source": [
    "- Load the `en_core_web_sm` pipeline and create the `nlp` object.\n",
    "- Print the names of the pipeline components using `nlp.pipe_names`.\n",
    "- Print the full pipeline of `(name, component)` tuples using `nlp.pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "219811db-c089-41ca-9a4c-de33ab14af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7d470a3e5430>)\n",
      "('tagger', <spacy.pipeline.tagger.Tagger object at 0x7d470a3e72f0>)\n",
      "('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7d470a217df0>)\n",
      "('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7d47071a85d0>)\n",
      "('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7d470a44ca10>)\n",
      "('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7d470a217ca0>)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "for x in nlp.pipeline:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc455f54-9b6f-4ddc-a1c7-14fbd5620b01",
   "metadata": {},
   "source": [
    "## Custom pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de111e2-2395-4df7-bf76-42bb884bb7f7",
   "metadata": {},
   "source": [
    "### Why custom components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5b8ec8-0b32-4e00-89c4-d50fab3e94f4",
   "metadata": {},
   "source": [
    "After the text is tokenized and a `Doc` object has been created, pipeline components are applied in order. spaCy supports a range of built-in components, but also lets you define your own.\n",
    "\n",
    "Custom components are executed automatically when you call the `nlp` object on a text.\n",
    "\n",
    "They're especially useful for adding your own custom metadata to documents and tokens.\n",
    "\n",
    "You can also use them to update built-in attributes, like the named entity spans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec8075-3836-442d-ab2d-95a0924e069a",
   "metadata": {},
   "source": [
    "![pipeline](./img/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0188f-661a-405c-8e8d-d06d6e35499c",
   "metadata": {},
   "source": [
    "- Make a function execute automatically when you call `nlp`\n",
    "- Add your own metadata to documents and tokens\n",
    "- Updating built-in attributes like `doc.ents`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e135087-2963-44ca-8d10-b3a15a45e038",
   "metadata": {},
   "source": [
    "### Anatomy of a component(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba781d8-0813-4a52-8f21-3799064dc60c",
   "metadata": {},
   "source": [
    "Fundamentally, a pipeline component is a function or callable that takes a doc, modifies it and returns it, so it can be processed by the next component in the pipeline.\n",
    "\n",
    "To tell spaCy where to find your custom component and how it should be called, you can decorate it using the `@Language.component` decorator. Just add it to the line right above the function definition.\n",
    "\n",
    "Once a component is registered, it can be added to the pipeline using the `nlp.add_pipe` method. The method takes at least one argument: the string name of the component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00654f3e-0d33-4d66-87ef-ebacf52f38e4",
   "metadata": {},
   "source": [
    "- Function that takes a `doc`, modifies it and returns it\n",
    "- Registered using the `Language.component` decorator\n",
    "- Can be added using the `nlp.add_pipe` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b71756c8-86e7-473f-a42e-3f33fd232118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.custom_component_function(doc)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Do something to the doc here\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"custom_component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711fa48-f457-44ed-9cc9-31fec241d771",
   "metadata": {},
   "source": [
    "### Anatomy of a component(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a41fd6-ca2a-48c1-98ce-fe196a8c6fb5",
   "metadata": {},
   "source": [
    "To specify *where* to add the component in the pipeline, you can use the following keyword arguments:\n",
    "\n",
    "Setting `last` to `True` will add the component last in the pipeline. This is the default behavior.\n",
    "\n",
    "Setting `first` to `True` will add the component first in the pipeline, right after the tokenizer.\n",
    "\n",
    "The `before` and `after` arguments let you define the name of an existing component to add the new component before or after. For example, `before=\"ner\"` will add it before the named entity recognizer.\n",
    "\n",
    "The other component to add the new component before or after needs to exist, though – otherwise, spaCy will raise an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87da9b13-31ac-4d80-a6bd-91dde9faad36",
   "metadata": {},
   "source": [
    "Argument | Description | Example\n",
    "-----------|-----------|--------\n",
    "last | If True, add last | nlp.add_pipe(\"component\", last=True)\n",
    "first | If True, add first | nlp.add_pipe(\"component\", first=True)\n",
    "before | Add before component | nlp.add_pipe(\"component\", before=\"ner\")\n",
    "after | Add after component | nlp.add_pipe(\"component\", after=\"tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ce1df-eef5-4036-9c69-63c5a00a1cae",
   "metadata": {},
   "source": [
    "### Example: a simple component(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89498c-84cf-4622-8a30-d60cee376e27",
   "metadata": {},
   "source": [
    "Here's an example of a simple pipeline component.\n",
    "\n",
    "We start off with the small English pipeline.\n",
    "\n",
    "We then define the component – a function that takes a `Doc` object and returns it.\n",
    "\n",
    "Let's do something simple and print the length of the doc that passes through the pipeline.\n",
    "\n",
    "Don't forget to return the doc so it can be processed by the next component in the pipeline! The doc created by the tokenizer is passed through all components, so it's important that they all return the modified doc.\n",
    "\n",
    "To tell spaCy about the new component, we register it using the `@Language.component` decorator and call it \"custom_component\".\n",
    "\n",
    "We can now add the component to the pipeline. Let's add it to the very beginning right after the tokenizer by setting `first=True`.\n",
    "\n",
    "When we print the pipeline component names, the custom component now shows up at the start. This means it will be applied first when we process a doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bf35847-2b7a-4d3b-8610-e2f87dd06e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08e25c-a2f8-4a47-8a1b-df9c025ba2e3",
   "metadata": {},
   "source": [
    "### Example: a simple component(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16bd062-5d76-4db2-99cd-4fb6a85cab95",
   "metadata": {},
   "source": [
    "Now when we process a text using the nlp object, the custom component will be applied to the doc and the length of the document will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b9370e-399e-4bcc-8102-c059bc2617d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363c5c6-bc51-45eb-9a27-1b6f5c500a01",
   "metadata": {},
   "source": [
    "## Use cases for custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920aa88d-117e-4c39-b66e-8e58ed23f327",
   "metadata": {},
   "source": [
    "Which of these problems can be solved by custom pipeline components? Choose all that apply!\n",
    "\n",
    "1. Updating the trained pipelines and improving their predictions\n",
    "2. Computing your own values based on tokens and their attributes\n",
    "3. Adding named entities, for example based on a dictionary\n",
    "4. Implementing support for an additional language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04180a22-ef53-4b60-ab1c-6e902e1f29bf",
   "metadata": {},
   "source": [
    "Answer: 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e71a4-4271-41c0-9481-680c6fbeba24",
   "metadata": {},
   "source": [
    "Custom components are great for adding custom values to documents, tokens and spans, and customizing the `doc.ents`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0272c6-0bc4-4965-9d3d-5adac72fde64",
   "metadata": {},
   "source": [
    "## Simple components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8f1dc-9c7d-499d-a8bb-e7f92ee8c358",
   "metadata": {},
   "source": [
    "The example shows a custom component that prints the number of tokens in a document. Can you complete it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096c838-c1b5-44fd-9c78-a219659d530a",
   "metadata": {},
   "source": [
    "- Complete the component function with the `doc`’s length.\n",
    "- Add the `\"length_component\"` to the existing pipeline as the **first** component.\n",
    "- Try out the new pipeline and process any text with the `nlp` object – for example “This is a sentence.”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34122230-313e-4b1d-92e4-9b3befec643a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "This document is 4 tokens long.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"length_component\")\n",
    "def length_component_function(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3c0e4-cfa9-4329-bcb9-373a10134368",
   "metadata": {},
   "source": [
    "## Complex components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69437cd2-5ee9-4267-81ff-bc1cfb557a46",
   "metadata": {},
   "source": [
    "In this exercise, you’ll be writing a custom component that uses the `PhraseMatcher` to find animal names in the document and adds the matched spans to the `doc.ents`. A `PhraseMatcher` with the animal patterns has already been created as the variable `matcher`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa10d38-2c3c-4237-91b4-2eb43a43c9f3",
   "metadata": {},
   "source": [
    "- Define the custom component and apply the `matcher` to the `doc`.\n",
    "- Create a `Span` for each match, assign the label ID for `\"ANIMAL\"` and overwrite the `doc.ents` with the new spans.\n",
    "- Add the new component to the pipeline *after* the `\"ner\"` component.\n",
    "- Process the text and print the entity text and entity label for the entities in `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05734a2-24ce-4d8b-94b1-3600a8d61421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
